{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af7fdcea-79bb-4b00-a78a-8828042a19cf",
   "metadata": {},
   "source": [
    "- https://x.com/karpathy/status/1691571869051445433"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce67792-aaaa-446d-abdb-c32ba111c1eb",
   "metadata": {},
   "source": [
    "llama.cpp surprised many people (myself included) with how quickly you can run large LLMs on small computers, e.g. 7B runs @ ~16 tok/s on a MacBook. Wait don't you need supercomputers to work with LLMs?\n",
    "\n",
    "TLDR at **batch_size=1** (i.e. just generating a single stream of prediction on your computer), the inference is super duper memory-bound. The on-chip compute units are twiddling their thumbs while sucking model weights through a straw from DRAM. Every individual weight that is expensively loaded from DRAM onto the chip is only used for a single instant multiply to process each new input token. So the stat to look at is not FLOPS but the memory bandwidth.\n",
    "\n",
    "Let's take a look:\n",
    "A100: 1935 GB/s memory bandwidth, 1248 TOPS\n",
    "MacBook M2: 100 GB/s, 7 TFLOPS\n",
    "\n",
    "The compute is ~200X but the memory bandwidth only ~20X. So the little M2 chip that could will only be about ~20X slower than a mighty A100. This is ~10X faster than you might naively expect just looking at ops.\n",
    "\n",
    "The situation becomes a lot more different when you inference at a very **high batch size (e.g. ~160+)**, such as when you're hosting an LLM engine simultaneously serving a lot of parallel requests. Or in training, where you aren't forced to go serially token by token and can parallelize across both batch and time dimension, because the next token targets (labels) are known. In these cases, once you load the weights into on-chip cache and pay that large fixed cost, you can re-use them across many input examples and reach ~50%+ utilization, actually making those FLOPS count.\n",
    "\n",
    "So TLDR why is LLM inference surprisingly fast on your MacBook? If all you want to do is batch 1 inference (i.e. a single \"stream\" of generation), only the memory bandwidth matters. And the memory bandwidth gap between chips is a lot smaller, and has been a lot harder to scale compared to flops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b61eba-fa0d-447a-8af6-e99f90467b82",
   "metadata": {},
   "source": [
    "- TLDR at batch_size=1 (i.e. just generating a single stream of prediction on your computer), the inference is super duper **memory-bound**. The on-chip compute units are twiddling their thumbs while sucking model weights through a straw from DRAM. Every individual weight that is expensively loaded from DRAM onto the chip is only used for a single instant multiply to process each new input token. So the stat to look at is not FLOPS but the **memory bandwidth**.\n",
    "    - compute-bound：flops\n",
    "    - memory-bound：memory bandwidth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a42083d-8915-4959-ad36-fa757747d8a5",
   "metadata": {},
   "source": [
    "```\n",
    "# optional\n",
    "conda create -n llama3 python=3.11\n",
    "conda activate llama3\n",
    "\n",
    "# hf version llama3 weights\n",
    "git lfs install\n",
    "git clone https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\n",
    "\n",
    "\n",
    "# hf cli\n",
    "pip3 install -U \"huggingface_hub[cli]\" \n",
    "huggingface-cli login\n",
    "\n",
    "\n",
    "# clone & make llama.cpp\n",
    "git clone https://github.com/ggerganov/llama.cpp \n",
    "cd llama.cpp \n",
    "make          # make GGML_CUDA=1 -j8\n",
    "python3 -m pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "- gpu (cuda) 版本\n",
    "    - `./llama-cli -h | grep -i ngl`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9b6d99-3ae3-41a3-b3a2-5bc0e69f537f",
   "metadata": {},
   "source": [
    "- 转换：hf => gguf\n",
    "    ```\n",
    "    python3 convert-hf-to-gguf.py /your-folder/Meta-Llama-3-8B-Instruct\n",
    "    ```\n",
    "    - `n_tensors = 291, total_size = 16.1G`\n",
    "    - `ggml-model-f16.gguf`\n",
    "- Quantize the Model\n",
    "    ```\n",
    "    ./llm_quantize /your-folder/Meta-Llama-3-8B-Instruct/ggml-model-f16.gguf q4_0\n",
    "    ```\n",
    "    - `ggml-model-Q4_0.gguf`\n",
    "- Run the Model：走的是 memory + cpu (计算) 模式；（`-ngl 40` => gpu/cuda）\n",
    "    ```\n",
    "    ./llama-cli -m /your-folder/Meta-Llama-3-8B-Instruct/ggml-model-Q4_0.gguf -n 256 --repeat_penalty 1.0 --color -i -r \"User:\" -f prompts/chat-with-bob.txt\n",
    "    # chatgpt like chat\n",
    "    ./llama-cli -m your_model.gguf -p \"You are a helpful assistant\" -cnv\n",
    "    # web\n",
    "    ./llama-server -m your_model.gguf --port 8080\n",
    "    ```\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
