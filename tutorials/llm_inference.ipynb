{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "011b0515-313b-471d-9cc4-f5451ff5c62c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T12:03:32.461076Z",
     "iopub.status.busy": "2024-11-17T12:03:32.460439Z",
     "iopub.status.idle": "2024-11-17T12:03:32.470768Z",
     "shell.execute_reply": "2024-11-17T12:03:32.468993Z",
     "shell.execute_reply.started": "2024-11-17T12:03:32.461028Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec1d302-323f-4035-8723-ccb9eb30312b",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=hMs8VNRy5Ys\n",
    "- nvidia\n",
    "    - https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/\n",
    "    - https://res.cloudinary.com/dyd911kmh/image/upload/v1713882586/Marketing/webinars/Slides/dataacamp-llm-inference-webinar.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcf585a-27c8-4177-ac37-eee6c63347fb",
   "metadata": {},
   "source": [
    "- decoder-only inference\n",
    "    - GPT-like models\n",
    "    - No encoder, no encoder-decoder multi-head attention\n",
    "    - input processing (aka **prefill**): highly parallel\n",
    "        - input (the tokenized prompt) are embedded and encoded\n",
    "        - mha computes the keys and values (KV)\n",
    "        - large matrix multiplication, high usage of the hardware accelerator\n",
    "    - output generation: sequential\n",
    "        - the answer is generated **one token** at a time.\n",
    "        - each generated token is **appended** to the previous input\n",
    "        - the process is repeated until the stopping criteria is met\n",
    "            - max length or EOS\n",
    "        - low usage of the hardware accelerator\n",
    "- initial prompt processing (Prefill):\n",
    "    - The phase where the model gets to understand the input prompt\n",
    "- Decode Phase – Generating one token at a time\n",
    "    - Decoding is sped up because of the KV Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec5989cf-41ba-4442-99e2-7c15d68b2d9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T10:59:57.186303Z",
     "iopub.status.busy": "2024-11-16T10:59:57.185712Z",
     "iopub.status.idle": "2024-11-16T10:59:57.204109Z",
     "shell.execute_reply": "2024-11-16T10:59:57.202370Z",
     "shell.execute_reply.started": "2024-11-16T10:59:57.186256Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./imgs/decoder-only.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='./imgs/decoder-only.png', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dd4395-adbc-4932-95a0-2c98cb008718",
   "metadata": {},
   "source": [
    "### measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bd5bf4-add4-4b02-91c7-cb300987babd",
   "metadata": {},
   "source": [
    "- TTFT：Time to first token\n",
    "- Inter-token latency\n",
    "- total time to generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af423ac5-5e86-4129-acc0-c463e2bea1ca",
   "metadata": {},
   "source": [
    "### kv cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cb27dd-b6fb-45de-87f0-29d264957af1",
   "metadata": {},
   "source": [
    "- https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/\n",
    "- https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/transformers-neuronx/generative-llm-inference-with-neuron.html\n",
    "- cache size (fp16)\n",
    "    - `2*2*bs*seq_length*num_layers*d_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "134bfb88-4a76-41af-81f2-0e911c6076c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T12:03:40.895115Z",
     "iopub.status.busy": "2024-11-17T12:03:40.894815Z",
     "iopub.status.idle": "2024-11-17T12:03:40.907624Z",
     "shell.execute_reply": "2024-11-17T12:03:40.905548Z",
     "shell.execute_reply.started": "2024-11-17T12:03:40.895095Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/key-value-caching_.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/key-value-caching_.png', width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4d18771-b3ba-4a24-b498-961d0f3312f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T11:10:54.597370Z",
     "iopub.status.busy": "2024-11-16T11:10:54.596764Z",
     "iopub.status.idle": "2024-11-16T11:10:54.609542Z",
     "shell.execute_reply": "2024-11-16T11:10:54.607211Z",
     "shell.execute_reply.started": "2024-11-16T11:10:54.597325Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://awsdocs-neuron.readthedocs-hosted.com/en/latest/_images/masked-self-attention-operator.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://awsdocs-neuron.readthedocs-hosted.com/en/latest/_images/masked-self-attention-operator.png', width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29f92952-60a9-4c12-abb4-191be133a728",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T11:11:06.627291Z",
     "iopub.status.busy": "2024-11-16T11:11:06.626706Z",
     "iopub.status.idle": "2024-11-16T11:11:06.639500Z",
     "shell.execute_reply": "2024-11-16T11:11:06.637217Z",
     "shell.execute_reply.started": "2024-11-16T11:11:06.627244Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://awsdocs-neuron.readthedocs-hosted.com/en/latest/_images/kv-cache-optimization.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://awsdocs-neuron.readthedocs-hosted.com/en/latest/_images/kv-cache-optimization.png', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa22cda1-8135-4bd6-857f-4a66f34dcf99",
   "metadata": {},
   "source": [
    "### continuous batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a317c29f-a1e2-4d53-a090-44b295582dba",
   "metadata": {},
   "source": [
    "- https://www.anyscale.com/blog/continuous-batching-llm-inference\n",
    "- Decoder-only inference requests are harder to batch than for traditional Transformers\n",
    "- Input and output lengths can greatly vary, leading to very different generation times\n",
    "- traditional batching waits for all requests to complete\n",
    "- continuous batching evicts completed requests and runs new requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8c90873-d245-4440-952d-c98916efe52d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T11:16:07.394830Z",
     "iopub.status.busy": "2024-11-16T11:16:07.394245Z",
     "iopub.status.idle": "2024-11-16T11:16:07.406827Z",
     "shell.execute_reply": "2024-11-16T11:16:07.404849Z",
     "shell.execute_reply.started": "2024-11-16T11:16:07.394782Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://images.ctfassets.net/xjan103pcp94/1LJioEsEdQQpDCxYNWirU6/82b9fbfc5b78b10c1d4508b60e72fdcf/cb_02_diagram-static-batching.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://images.ctfassets.net/xjan103pcp94/1LJioEsEdQQpDCxYNWirU6/82b9fbfc5b78b10c1d4508b60e72fdcf/cb_02_diagram-static-batching.png', width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9324402f-ee9a-4644-b6b6-64a5d91026c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T11:17:48.814554Z",
     "iopub.status.busy": "2024-11-16T11:17:48.813964Z",
     "iopub.status.idle": "2024-11-16T11:17:48.826310Z",
     "shell.execute_reply": "2024-11-16T11:17:48.824381Z",
     "shell.execute_reply.started": "2024-11-16T11:17:48.814509Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://images.ctfassets.net/xjan103pcp94/744TAv4dJIQqeHcEaz5lko/b823cc2d92bbb0d82eb252901e1dce6d/cb_03_diagram-continuous-batching.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://images.ctfassets.net/xjan103pcp94/744TAv4dJIQqeHcEaz5lko/b823cc2d92bbb0d82eb252901e1dce6d/cb_03_diagram-continuous-batching.png', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae694556-a4a7-4fe2-b67e-cd6873c4d666",
   "metadata": {},
   "source": [
    "### speculative decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1465075a-b589-4f6f-95ec-a474b35aeb12",
   "metadata": {},
   "source": [
    "- https://huggingface.co/blog/assisted-generation\n",
    "- https://github.com/huggingface/transformers/blob/849367ccf741d8c58aa88ccfe1d52d8636eaf2b7/src/transformers/generation/utils.py#L4064\n",
    "- the two models must share the same tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f95c4a94-e377-470e-916f-0c0db9a57956",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T11:23:54.734010Z",
     "iopub.status.busy": "2024-11-16T11:23:54.733682Z",
     "iopub.status.idle": "2024-11-16T11:23:54.741546Z",
     "shell.execute_reply": "2024-11-16T11:23:54.739340Z",
     "shell.execute_reply.started": "2024-11-16T11:23:54.733987Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:7890'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad3f5b34-28fb-428f-ad9c-b65adda61063",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T11:23:57.986957Z",
     "iopub.status.busy": "2024-11-16T11:23:57.986354Z",
     "iopub.status.idle": "2024-11-16T11:25:21.668454Z",
     "shell.execute_reply": "2024-11-16T11:25:21.667064Z",
     "shell.execute_reply.started": "2024-11-16T11:23:57.986910Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dcfc5b37f554ed88b7d48f739f08e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ff38b0f54f40799531a1c7f5f04486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc708ce5dcd405bb5c71a6707f58fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whaow/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1602: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa14d0172f444111a297eb0c2000583e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a8c9f86c9a4142a5fa5afad75e5c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f2186617514168a8ee05b7bc837051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/569 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b52695aa0b444643a3fd7c7d8c804b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1233: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alice and Bob are sitting in a bar. Alice is drinking a beer and Bob is drinking a']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "prompt = \"Alice and Bob\"\n",
    "checkpoint = \"EleutherAI/pythia-1.4b-deduped\"\n",
    "assistant_checkpoint = \"EleutherAI/pythia-160m-deduped\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "assistant_model = AutoModelForCausalLM.from_pretrained(assistant_checkpoint).to(device)\n",
    "outputs = model.generate(**inputs, assistant_model=assistant_model)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "# ['Alice and Bob are sitting in a bar. Alice is drinking a beer and Bob is drinking a']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41108df8-a915-42c2-9baf-0b2459d59ba8",
   "metadata": {},
   "source": [
    "### speculative decoding：n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aec94ec-9ed1-4824-8bb6-0ea491d55e06",
   "metadata": {},
   "source": [
    "- https://github.com/apoorvumang/prompt-lookup-decoding\n",
    "    - prompt lookup algorithm\n",
    "- https://twitter.com/joao_gante/status/1747322418425741550\n",
    "- Input-grounded tasks (summarization, document QA, multi-turn chat, code editing):\n",
    "    - **high n-gram overlap** between the input (prompt) and the generated output\n",
    "- We can use strings present in the prompt to generate candidate token sequences\n",
    "- Significant speedups (2x-4x), without model modification and with no effect on output quality\n",
    "- Implemented in the transformers library\n",
    "```\n",
    "generation_output = model.generate(\n",
    "    **input_ids, \n",
    "    do_sample=False, \n",
    "    max_new_tokens=512, \n",
    "    streamer=streamer, \n",
    "    prompt_lookup_num_tokens=10\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b36a89-4e2e-4592-90a0-655d7966634d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
