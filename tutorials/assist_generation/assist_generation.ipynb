{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c3a097b-715f-406e-8ced-4828a6f21346",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T03:52:52.238168Z",
     "iopub.status.busy": "2024-11-17T03:52:52.237570Z",
     "iopub.status.idle": "2024-11-17T03:52:52.249299Z",
     "shell.execute_reply": "2024-11-17T03:52:52.248270Z",
     "shell.execute_reply.started": "2024-11-17T03:52:52.238137Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:7890'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be498a59-dbe6-416d-bc20-d1a556f7d3d3",
   "metadata": {},
   "source": [
    "### speculative decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53192a3e-bb6d-47dd-a55c-a08d3224bd09",
   "metadata": {},
   "source": [
    "- https://huggingface.co/blog/assisted-generation\n",
    "- https://github.com/huggingface/transformers/blob/849367ccf741d8c58aa88ccfe1d52d8636eaf2b7/src/transformers/generation/utils.py#L4064\n",
    "- the two models must share the same tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0bb7d6c-a9fe-4d43-ac87-6d0fc4889ffc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T03:52:52.250865Z",
     "iopub.status.busy": "2024-11-17T03:52:52.250533Z",
     "iopub.status.idle": "2024-11-17T03:52:59.189467Z",
     "shell.execute_reply": "2024-11-17T03:52:59.188652Z",
     "shell.execute_reply.started": "2024-11-17T03:52:52.250838Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whaow/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1602: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1233: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "prompt = \"Alice and Bob\"\n",
    "checkpoint = \"EleutherAI/pythia-1.4b-deduped\"\n",
    "assistant_checkpoint = \"EleutherAI/pythia-160m-deduped\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "assistant_model = AutoModelForCausalLM.from_pretrained(assistant_checkpoint).to(device)\n",
    "outputs = model.generate(**inputs, assistant_model=assistant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff29a932-14e2-4507-8e1c-b0f8094a984c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T03:52:59.190455Z",
     "iopub.status.busy": "2024-11-17T03:52:59.190240Z",
     "iopub.status.idle": "2024-11-17T03:52:59.198118Z",
     "shell.execute_reply": "2024-11-17T03:52:59.197296Z",
     "shell.execute_reply.started": "2024-11-17T03:52:59.190443Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alice and Bob are sitting in a bar. Alice is drinking a beer and Bob is drinking a']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "# ['Alice and Bob are sitting in a bar. Alice is drinking a beer and Bob is drinking a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8b7bf50-9f54-4279-988a-c91b5c182969",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T03:57:03.797260Z",
     "iopub.status.busy": "2024-11-17T03:57:03.796627Z",
     "iopub.status.idle": "2024-11-17T03:57:03.819457Z",
     "shell.execute_reply": "2024-11-17T03:57:03.817348Z",
     "shell.execute_reply.started": "2024-11-17T03:57:03.797213Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[2422,  547,  285, 8679]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1]], device='cuda:0')},\n",
       " torch.Size([1, 4]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bs * seq_len\n",
    "inputs, inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00850a97-1c6d-428c-978e-08f5f7762aac",
   "metadata": {},
   "source": [
    "#### assistant_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "470b2fb7-ba42-4e2d-966b-d8b2ef7a74ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T04:33:27.659210Z",
     "iopub.status.busy": "2024-11-17T04:33:27.658619Z",
     "iopub.status.idle": "2024-11-17T04:33:27.673600Z",
     "shell.execute_reply": "2024-11-17T04:33:27.671277Z",
     "shell.execute_reply.started": "2024-11-17T04:33:27.659166Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXConfig {\n",
       "  \"_name_or_path\": \"EleutherAI/pythia-160m-deduped\",\n",
       "  \"architectures\": [\n",
       "    \"GPTNeoXForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": true,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": 0.1,\n",
       "  \"eos_token_id\": 0,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout\": 0.0,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"model_type\": \"gpt_neox\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rotary_emb_base\": 10000,\n",
       "  \"rotary_pct\": 0.25,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.45.0.dev0\",\n",
       "  \"use_cache\": true,\n",
       "  \"use_parallel_residual\": true,\n",
       "  \"vocab_size\": 50304\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"vocab_size\": 50304\n",
    "# \"hidden_size\": 768,\n",
    "# \"num_attention_heads\": 12,\n",
    "# \"num_hidden_layers\": 12,\n",
    "assistant_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "678365ef-375c-436d-8759-222b1f1e8d60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T04:28:45.072616Z",
     "iopub.status.busy": "2024-11-17T04:28:45.071960Z",
     "iopub.status.idle": "2024-11-17T04:28:45.084683Z",
     "shell.execute_reply": "2024-11-17T04:28:45.082228Z",
     "shell.execute_reply.started": "2024-11-17T04:28:45.072568Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "768 / 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d781c316-e985-490e-92c7-b83f5268f277",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T04:25:49.456884Z",
     "iopub.status.busy": "2024-11-17T04:25:49.456269Z",
     "iopub.status.idle": "2024-11-17T04:25:49.566648Z",
     "shell.execute_reply": "2024-11-17T04:25:49.564754Z",
     "shell.execute_reply.started": "2024-11-17T04:25:49.456839Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Alice and Bob, who were both in']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(assistant_model.generate(input_ids=inputs['input_ids'], \n",
    "                                                min_new_tokens=0, \n",
    "                                                max_new_tokens=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b12452-5371-4aec-9b2f-68c438180043",
   "metadata": {},
   "source": [
    "### config & kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820dd189-6a4b-46dd-b739-3ed982a5b8bd",
   "metadata": {},
   "source": [
    "- `LogitsProcessorList`\n",
    "- cache_name: `past_key_values` (kv-cache)\n",
    "    - `model_kwargs['past_key_values'] = DynamicCache()`\n",
    "- model_input_name: `input_ids`\n",
    "    - model_kwargs\n",
    "        - attention_mask\n",
    "        - use_cache=True\n",
    "        - past_key_values: 通过 `DynamicCache()` 的实例来存放；\n",
    "- `generation_mode == GenerationMode.ASSISTED_GENERATION`\n",
    "    - assisted generate is only supported for batch_size = 1\n",
    "    - assisted generate requires `use_cache=True`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd3b734-0fd2-4bbe-8a40-498003b68c1e",
   "metadata": {},
   "source": [
    "- cache\n",
    "    - `cache_position = torch.ones_like(input_ids[0, :], dtype=torch.int64).cumsum(0) - 1`\n",
    "    - `DynamicCache()`\n",
    "        - `past_length = cache.get_seq_length()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8a3cacb-590c-4825-b7ff-ff00eafd01d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T04:19:41.903461Z",
     "iopub.status.busy": "2024-11-17T04:19:41.902819Z",
     "iopub.status.idle": "2024-11-17T04:19:41.917791Z",
     "shell.execute_reply": "2024-11-17T04:19:41.915674Z",
     "shell.execute_reply.started": "2024-11-17T04:19:41.903414Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones_like(inputs['input_ids'][0, :]).cumsum(0) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c78cecb-6060-42bc-9a59-342b07961743",
   "metadata": {},
   "source": [
    "### `AssistedCandidateGenerator`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612239f7-ea8d-4b23-a634-157542e74a00",
   "metadata": {},
   "source": [
    "`def get_candidates(self, input_ids: torch.LongTensor)` => `candidate_logits, candidate_ids`\n",
    "- candidate_input_ids.shape: [1, 9]\n",
    "- candidate_logits.shape: [1, 5, 50304]\n",
    "\n",
    "candidate_length = candidate_input_ids.shape[1] - input_ids.shape[1]\n",
    "- 9-4=5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
