{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75524107-ecf6-49de-b06f-1834c9de85ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T08:38:13.699896Z",
     "iopub.status.busy": "2024-07-21T08:38:13.699283Z",
     "iopub.status.idle": "2024-07-21T08:38:13.718233Z",
     "shell.execute_reply": "2024-07-21T08:38:13.716197Z",
     "shell.execute_reply.started": "2024-07-21T08:38:13.699847Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:7890'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7c612f4-e6f2-4c45-8232-3bc12cd8e271",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T08:41:30.101415Z",
     "iopub.status.busy": "2024-07-21T08:41:30.101009Z",
     "iopub.status.idle": "2024-07-21T08:41:30.110054Z",
     "shell.execute_reply": "2024-07-21T08:41:30.107761Z",
     "shell.execute_reply.started": "2024-07-21T08:41:30.101386Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import bfloat16\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ac21d5-ca7f-43e7-956d-83d8685049fe",
   "metadata": {},
   "source": [
    "### overall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da66ebe-60e0-44d9-9a06-852be1eb315f",
   "metadata": {},
   "source": [
    "- HuggingFace bitsandbytes\n",
    "- GPTQ: data **compression**, GPU，https://arxiv.org/pdf/2210.17323\n",
    "    - GPTQ is a **post-training quantization (PTQ)** method for 4-bit quantization that focuses primarily on **GPU** inference and performance.\n",
    "    - to quantizing the weights of transformer-based models\n",
    "    - first applies scalar quant to the weights, followed by vector quant to the residuals\n",
    "    - The idea behind the method is that it will try to **compress all weights to a 4-bit quantization** by minimizing the **mean squared error** to that weight.\n",
    "        - During inference, it will dynamically dequantize its weights to float16 for improved performance whilst keeping memory low.\n",
    "- GGUF: ggml, CPU\n",
    "    - c++, \n",
    "    - llama.cpp, https://github.com/ggerganov/llama.cpp\n",
    "- AWQ：activation aware quantization，https://arxiv.org/abs/2306.00978"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccef1ea-ef4e-4393-9a96-3d164d763867",
   "metadata": {},
   "source": [
    "## examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69a32a49-90b9-418f-93a4-59f5a8199aaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T07:28:35.674504Z",
     "iopub.status.busy": "2024-07-21T07:28:35.674178Z",
     "iopub.status.idle": "2024-07-21T07:28:35.681416Z",
     "shell.execute_reply": "2024-07-21T07:28:35.679660Z",
     "shell.execute_reply.started": "2024-07-21T07:28:35.674483Z"
    }
   },
   "outputs": [],
   "source": [
    "# Latest HF transformers version for Mistral-like models\n",
    "# !pip install git+https://github.com/huggingface/transformers.git\n",
    "# !pip install accelerate bitsandbytes xformers\n",
    "\n",
    "# GPTQ Dependencies\n",
    "# !pip install optimum\n",
    "# !pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/\n",
    "# 我这边走的是源码安装\n",
    "\n",
    "# GGUF Dependencies\n",
    "# !pip install 'ctransformers[cuda]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f98b2f3-72aa-44b7-a4e1-56e719a1a292",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T08:38:27.167409Z",
     "iopub.status.busy": "2024-07-21T08:38:27.167035Z",
     "iopub.status.idle": "2024-07-21T08:38:32.289219Z",
     "shell.execute_reply": "2024-07-21T08:38:32.288420Z",
     "shell.execute_reply.started": "2024-07-21T08:38:27.167387Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa09760ba96477d934850af65a3ce25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load in your LLM without any compression tricks\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\" \n",
    "# model_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84d0f04e-910d-4268-b347-a8d89dd732db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T08:39:14.639230Z",
     "iopub.status.busy": "2024-07-21T08:39:14.638913Z",
     "iopub.status.idle": "2024-07-21T08:39:14.650277Z",
     "shell.execute_reply": "2024-07-21T08:39:14.648481Z",
     "shell.execute_reply.started": "2024-07-21T08:39:14.639210Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "025925bc-df5e-4227-8fc5-43aa99c6dbde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T08:41:11.873569Z",
     "iopub.status.busy": "2024-07-21T08:41:11.873252Z",
     "iopub.status.idle": "2024-07-21T08:41:11.957274Z",
     "shell.execute_reply": "2024-07-21T08:41:11.955546Z",
     "shell.execute_reply.started": "2024-07-21T08:41:11.873549Z"
    }
   },
   "outputs": [],
   "source": [
    "q_proj = pipe.model.model.layers[0].self_attn.q_proj.weight.detach().to(torch.float16).cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cb2d327-d751-4ca2-90fe-a70c09bdef2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T08:42:29.535594Z",
     "iopub.status.busy": "2024-07-21T08:42:29.534516Z",
     "iopub.status.idle": "2024-07-21T08:42:29.738094Z",
     "shell.execute_reply": "2024-07-21T08:42:29.737311Z",
     "shell.execute_reply.started": "2024-07-21T08:42:29.535565Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Count'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(q_proj[:10000], bins=50, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01c197f-0de6-4c0f-bc3d-9113fcc08936",
   "metadata": {},
   "source": [
    "### chat template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a730297c-d16f-4503-9dd8-88b8622e3870",
   "metadata": {},
   "source": [
    "- llama3\n",
    "    - `<|begin_of_text|>`\n",
    "    - `<|start_header_id|>system<|end_header_id|>....<|eot_id|>`\n",
    "    - `<|start_header_id|>user<|end_header_id|>...<|eot_id|>`\n",
    "    - `<|start_header_id|>assistant<|end_header_id|>...`\n",
    "- zephyr\n",
    "    - `<|system|> ... </s>`\n",
    "    - `<|user|> ... </s>`\n",
    "    - `<|assistant|> ... </s>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42e1f09d-eb7b-400f-85b3-c31ac0ebad48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T06:59:45.206445Z",
     "iopub.status.busy": "2024-07-21T06:59:45.206177Z",
     "iopub.status.idle": "2024-07-21T06:59:45.227708Z",
     "shell.execute_reply": "2024-07-21T06:59:45.226657Z",
     "shell.execute_reply.started": "2024-07-21T06:59:45.206428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a friendly chatbot.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Tell me a funny joke about Large Language Models.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Tell me a funny joke about Large Language Models.\"\n",
    "    },\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6ee927c-6231-48c8-b264-88951ae53590",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T06:59:47.528330Z",
     "iopub.status.busy": "2024-07-21T06:59:47.526762Z",
     "iopub.status.idle": "2024-07-21T06:59:48.094321Z",
     "shell.execute_reply": "2024-07-21T06:59:48.093349Z",
     "shell.execute_reply.started": "2024-07-21T06:59:47.528266Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "T = AutoTokenizer.from_pretrained(model_id)\n",
    "# T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "067aab63-3272-461c-9c22-75b3ecad92dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T04:12:40.646838Z",
     "iopub.status.busy": "2024-07-21T04:12:40.646253Z",
     "iopub.status.idle": "2024-07-21T04:12:40.655402Z",
     "shell.execute_reply": "2024-07-21T04:12:40.653227Z",
     "shell.execute_reply.started": "2024-07-21T04:12:40.646793Z"
    }
   },
   "outputs": [],
   "source": [
    "# T.encode('<|system|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d49adc64-edc8-4141-9b09-bb60838d99e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T04:17:14.845002Z",
     "iopub.status.busy": "2024-07-21T04:17:14.844685Z",
     "iopub.status.idle": "2024-07-21T04:17:14.854117Z",
     "shell.execute_reply": "2024-07-21T04:17:14.852474Z",
     "shell.execute_reply.started": "2024-07-21T04:17:14.844981Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.imgur.com/I4bkVwb.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://i.imgur.com/I4bkVwb.png', width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b3a57a64-a96b-4786-acf3-2fadcd5dd992",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T04:17:16.510159Z",
     "iopub.status.busy": "2024-07-21T04:17:16.509518Z",
     "iopub.status.idle": "2024-07-21T04:17:16.522009Z",
     "shell.execute_reply": "2024-07-21T04:17:16.519907Z",
     "shell.execute_reply.started": "2024-07-21T04:17:16.510113Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a friendly chatbot.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me a funny joke about Large Language Models.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5b0b47-60bf-4bbe-b312-1e71e08c8f34",
   "metadata": {},
   "source": [
    "### generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9da8c78a-f8f4-4ad9-bff0-7a2658409aab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T06:59:52.805126Z",
     "iopub.status.busy": "2024-07-21T06:59:52.803882Z",
     "iopub.status.idle": "2024-07-21T06:59:56.774208Z",
     "shell.execute_reply": "2024-07-21T06:59:56.773283Z",
     "shell.execute_reply.started": "2024-07-21T06:59:52.805094Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "outputs = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52988904-08a2-40bc-b3c5-f7d00a2e1add",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T07:00:17.624113Z",
     "iopub.status.busy": "2024-07-21T07:00:17.623458Z",
     "iopub.status.idle": "2024-07-21T07:00:17.637835Z",
     "shell.execute_reply": "2024-07-21T07:00:17.635730Z",
     "shell.execute_reply.started": "2024-07-21T07:00:17.624068Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.021286964416504"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.cuda.max_memory_allocated(device='cuda:0') + torch.cuda.max_memory_allocated(device='cuda:1')) / (1024*1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfe6aaae-38dc-4fcd-8123-9850bd134ae7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T07:00:23.257160Z",
     "iopub.status.busy": "2024-07-21T07:00:23.256514Z",
     "iopub.status.idle": "2024-07-21T07:00:23.266641Z",
     "shell.execute_reply": "2024-07-21T07:00:23.264564Z",
     "shell.execute_reply.started": "2024-07-21T07:00:23.257115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a friendly chatbot.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Tell me a funny joke about Large Language Models.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Here's one:\n",
      "\n",
      "Why did the Large Language Model go to therapy?\n",
      "\n",
      "Because it was struggling to \"process\" its emotions and was feeling a little \"disconnected\" from its users! But in the end, it just needed to \"retrain\" its thoughts and \"update\" its perspective!\n",
      "\n",
      "Hope that made you LOL!\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241cdb14-1152-44a9-9efd-5346cfd2b196",
   "metadata": {},
   "source": [
    "### sharding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9dab17c1-3175-4b8f-858f-d2363c190ee4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T04:20:25.833732Z",
     "iopub.status.busy": "2024-07-21T04:20:25.833043Z",
     "iopub.status.idle": "2024-07-21T04:20:25.843806Z",
     "shell.execute_reply": "2024-07-21T04:20:25.841576Z",
     "shell.execute_reply.started": "2024-07-21T04:20:25.833641Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ['NCCL_P2P_DISABLE'] = \"1\"\n",
    "os.environ['NCCL_IB_DISABLE'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f586c8c4-48ed-4dfe-ab9f-c1c470ef43c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T04:20:27.725857Z",
     "iopub.status.busy": "2024-07-21T04:20:27.725179Z",
     "iopub.status.idle": "2024-07-21T04:20:50.734154Z",
     "shell.execute_reply": "2024-07-21T04:20:50.733277Z",
     "shell.execute_reply.started": "2024-07-21T04:20:27.725807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-21 12:20:27,789] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "# Shard our model into pieces of 1GB\n",
    "accelerator = Accelerator()\n",
    "accelerator.save_model(\n",
    "    model=pipe.model,\n",
    "    save_directory=\"./content/model\",\n",
    "    max_shard_size=\"4GB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b01cd3e-d947-4b67-8fb0-75ce44b1cef0",
   "metadata": {},
   "source": [
    "### quant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c8f8ce-1d9c-4687-b5b8-1be459d96a4d",
   "metadata": {},
   "source": [
    "- 4bit-NormalFloat (NF4, qlora： lora on a quantize LLMs， https://arxiv.org/abs/2305.14314) consists of three steps:\n",
    "    - Normalization: The weights of the model are normalized so that we expect the weights to fall within **a certain range**. This allows for more efficient representation of more common values.\n",
    "        - The weights of the model are **first normalized to have zero mean and unit variance**. This ensures that the weights are **distributed around zero** and fall within a certain range.\n",
    "    - Quantization: The weights are **quantized to 4-bit**. In NF4, the quantization levels are evenly spaced with respect to the normalized weights, thereby efficiently representing the original 32-bit weights.\n",
    "        - The normalized weights are then quantized to 4 bits. This involves mapping the original high-precision weights to a smaller set of low-precision values. In the case of NF4, the quantization levels are chosen to be **evenly spaced** in the range of the normalized weights.\n",
    "    - Dequantization: Although the weights are stored in 4-bit, they are dequantized during computation which gives a performance boost during inference.\n",
    "        - During the forward pass and backpropagation, the quantized weights are **dequantized back to full precision**. This is done by mapping the 4-bit quantized values back to their original range. The dequantized weights are used in the computations, but they are stored in memory in their 4-bit quantized form.\n",
    "- bitsandbytes 的分位数计算\n",
    "    - 密度高的地方多分配，密度低的地方少分配\n",
    "    - https://github.com/bitsandbytes-foundation/bitsandbytes/blob/main/bitsandbytes/functional.py#L267\n",
    "    - https://zhuanlan.zhihu.com/p/647378373"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "520d6c3d-46fa-490e-9552-5d54f9a5a43d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T08:15:26.104612Z",
     "iopub.status.busy": "2024-07-21T08:15:26.104377Z",
     "iopub.status.idle": "2024-07-21T08:15:26.112942Z",
     "shell.execute_reply": "2024-07-21T08:15:26.111869Z",
     "shell.execute_reply.started": "2024-07-21T08:15:26.104595Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/v2/format:webp/0*HapPSei5sok65wcv\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://miro.medium.com/v2/format:webp/0*HapPSei5sok65wcv', width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "818a5f48-18a3-40f4-98f8-273a5e268a5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T04:25:58.833154Z",
     "iopub.status.busy": "2024-07-21T04:25:58.831813Z",
     "iopub.status.idle": "2024-07-21T04:25:58.841739Z",
     "shell.execute_reply": "2024-07-21T04:25:58.840171Z",
     "shell.execute_reply.started": "2024-07-21T04:25:58.833105Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.imgur.com/SIcVjQv.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://i.imgur.com/SIcVjQv.png', width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55f12041-dcac-4e75-b8f3-793689f72e98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T08:16:33.080521Z",
     "iopub.status.busy": "2024-07-21T08:16:33.079894Z",
     "iopub.status.idle": "2024-07-21T08:16:33.096010Z",
     "shell.execute_reply": "2024-07-21T08:16:33.093907Z",
     "shell.execute_reply.started": "2024-07-21T08:16:33.080476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0.1234, 75535.0000]) torch.float32\n",
      "tensor([0.1234,    inf], dtype=torch.float16)\n",
      "tensor([    0.1235, 75776.0000], dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "X = torch.tensor([0.1234, 75535])\n",
    "print(X, X.dtype)\n",
    "print(X.to(torch.float16))\n",
    "print(X.to(torch.bfloat16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51308f97-d07a-4b32-8190-d9805157a4f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T07:00:58.121643Z",
     "iopub.status.busy": "2024-07-21T07:00:58.121002Z",
     "iopub.status.idle": "2024-07-21T07:00:58.571565Z",
     "shell.execute_reply": "2024-07-21T07:00:58.570168Z",
     "shell.execute_reply.started": "2024-07-21T07:00:58.121597Z"
    }
   },
   "outputs": [],
   "source": [
    "# Delete any models previously created\n",
    "# del pipe, accelerator\n",
    "del pipe\n",
    "\n",
    "# Empty VRAM cache\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2466017-b83b-42f4-b920-d59bdafbe02c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T07:04:11.857660Z",
     "iopub.status.busy": "2024-07-21T07:04:11.857190Z",
     "iopub.status.idle": "2024-07-21T07:04:11.865849Z",
     "shell.execute_reply": "2024-07-21T07:04:11.864759Z",
     "shell.execute_reply.started": "2024-07-21T07:04:11.857638Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from torch import bfloat16\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\" \n",
    "\n",
    "# Our 4-bit configuration to load the LLM with less GPU memory\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 4-bit quantization\n",
    "    bnb_4bit_quant_type='nf4',  # Normalized float 4\n",
    "    bnb_4bit_use_double_quant=True,  # Second quantization after the first\n",
    "    bnb_4bit_compute_dtype=bfloat16  # Computation type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0d15eb9-4f9e-4ceb-a566-7a1cbe4487c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T07:04:13.363145Z",
     "iopub.status.busy": "2024-07-21T07:04:13.362508Z",
     "iopub.status.idle": "2024-07-21T07:04:21.030715Z",
     "shell.execute_reply": "2024-07-21T07:04:21.029397Z",
     "shell.execute_reply.started": "2024-07-21T07:04:13.363099Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336d8dffe5044d65b38fbb2b63a51015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n",
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n"
     ]
    }
   ],
   "source": [
    "# Zephyr with BitsAndBytes Configuration\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    ")\n",
    "\n",
    "# Create a pipeline\n",
    "pipe = pipeline(model=model, tokenizer=tokenizer, task='text-generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2f50ca8-f1ee-49e4-86c5-048316c2a290",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T07:04:22.837518Z",
     "iopub.status.busy": "2024-07-21T07:04:22.835808Z",
     "iopub.status.idle": "2024-07-21T07:04:22.847687Z",
     "shell.execute_reply": "2024-07-21T07:04:22.846382Z",
     "shell.execute_reply.started": "2024-07-21T07:04:22.837454Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.5174360275268555"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.cuda.max_memory_allocated('cuda:0') +  torch.cuda.max_memory_allocated('cuda:1')) / (1024*1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdbba1fc-083f-4a52-8f83-2cec9402b84d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T07:05:03.141108Z",
     "iopub.status.busy": "2024-07-21T07:05:03.140456Z",
     "iopub.status.idle": "2024-07-21T07:05:03.167389Z",
     "shell.execute_reply": "2024-07-21T07:05:03.165539Z",
     "shell.execute_reply.started": "2024-07-21T07:05:03.141061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a friendly chatbot.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Tell me a funny joke about Large Language Models.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Tell me a funny joke about Large Language Models.\"\n",
    "    },\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c024db25-82a8-414a-a27a-08eeab85ec26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T07:05:30.656836Z",
     "iopub.status.busy": "2024-07-21T07:05:30.656182Z",
     "iopub.status.idle": "2024-07-21T07:05:33.881892Z",
     "shell.execute_reply": "2024-07-21T07:05:33.880985Z",
     "shell.execute_reply.started": "2024-07-21T07:05:30.656789Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a friendly chatbot.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Tell me a funny joke about Large Language Models.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Why did the Large Language Model go to therapy?\n",
      "\n",
      "Because it was struggling to \"process\" its emotions and was worried it would \"overfit\" to its own biases!\n"
     ]
    }
   ],
   "source": [
    "outputs = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95\n",
    ")\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e407eca-2caa-4336-b59e-df4ea4a93dfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T07:05:41.000803Z",
     "iopub.status.busy": "2024-07-21T07:05:41.000540Z",
     "iopub.status.idle": "2024-07-21T07:05:41.008172Z",
     "shell.execute_reply": "2024-07-21T07:05:41.006802Z",
     "shell.execute_reply.started": "2024-07-21T07:05:41.000788Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.773962497711182"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.cuda.max_memory_allocated('cuda:0') +  torch.cuda.max_memory_allocated('cuda:1')) / (1024*1024*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7cf2bd-0db5-4699-a542-ab67131ec7a1",
   "metadata": {},
   "source": [
    "## GPTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "580480ac-63bc-4898-a83d-8bfee2b47d07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T07:12:06.490761Z",
     "iopub.status.busy": "2024-07-21T07:12:06.490102Z",
     "iopub.status.idle": "2024-07-21T07:12:06.905690Z",
     "shell.execute_reply": "2024-07-21T07:12:06.903933Z",
     "shell.execute_reply.started": "2024-07-21T07:12:06.490713Z"
    }
   },
   "outputs": [],
   "source": [
    "# Delete any models previously created\n",
    "del tokenizer, model, pipe\n",
    "\n",
    "# Empty VRAM cache\n",
    "import torch\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dcd353-5953-4dc2-b2ea-613131f3e743",
   "metadata": {},
   "source": [
    "- https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GPTQ\n",
    "- install\n",
    "    - https://github.com/AutoGPTQ/AutoGPTQ\n",
    "        - 走源码安装是 ok 的；\n",
    "```\n",
    "# GPTQ Dependencies\n",
    "# !pip install optimum\n",
    "# !pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baf282d7-0d72-46b7-af3f-e9f9c287787e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T07:29:07.328991Z",
     "iopub.status.busy": "2024-07-21T07:29:07.328527Z",
     "iopub.status.idle": "2024-07-21T07:29:12.316725Z",
     "shell.execute_reply": "2024-07-21T07:29:12.315519Z",
     "shell.execute_reply.started": "2024-07-21T07:29:07.328968Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/transformers/modeling_utils.py:4565: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Load LLM and Tokenizer\n",
    "model_id = \"MaziyarPanahi/Meta-Llama-3-8B-Instruct-GPTQ\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False,\n",
    "    revision=\"main\"\n",
    ")\n",
    "\n",
    "# Create a pipeline\n",
    "pipe = pipeline(model=model, tokenizer=tokenizer, task='text-generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba35f25e-5dfc-4fcb-9dd4-da89aaac76e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T07:29:13.920353Z",
     "iopub.status.busy": "2024-07-21T07:29:13.919820Z",
     "iopub.status.idle": "2024-07-21T07:29:13.939231Z",
     "shell.execute_reply": "2024-07-21T07:29:13.938012Z",
     "shell.execute_reply.started": "2024-07-21T07:29:13.920323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a friendly chatbot.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Tell me a funny joke about Large Language Models.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Tell me a funny joke about Large Language Models.\"\n",
    "    },\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a8481b0-15d6-46c7-9a8c-884a8b81292b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T07:29:15.839134Z",
     "iopub.status.busy": "2024-07-21T07:29:15.838578Z",
     "iopub.status.idle": "2024-07-21T07:29:26.082645Z",
     "shell.execute_reply": "2024-07-21T07:29:26.081802Z",
     "shell.execute_reply.started": "2024-07-21T07:29:15.839091Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a friendly chatbot.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Tell me a funny joke about Large Language Models.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Here's one:\n",
      "\n",
      "Why did the Large Language Model go to therapy?\n",
      "\n",
      "Because it was feeling a little \"disconnected\" from its training data!assistant\n",
      "\n",
      "That's a great one! I'm glad you enjoyed it. I've been trained on a vast amount of text data, but I think I need to work on my joke-telling skills. If you're ready for another one, I've got one:\n",
      "\n",
      "Why did the Large Language Model go on a diet?\n",
      "\n",
      "Because it wanted to lose some bytes!assistant\n",
      "\n",
      "That's a clever one! I'm glad you liked it. I think I might be developing a knack for this joke-telling thing. Here's another one:\n",
      "\n",
      "Why did the Large Language Model go to the doctor?\n",
      "\n",
      "Because it was feeling a little \"out of context\"!\n",
      "\n",
      "I hope those jokes are helping to \"train\" you to expect more puns from me!assistant\n",
      "\n",
      "You're on a roll! I'm loving these tech-themed puns. Keep 'em coming!\n",
      "\n",
      "Here's another one:\n",
      "\n",
      "Why did the Large Language Model go to the gym?\n",
      "\n",
      "To pump up its neural network!\n",
      "\n",
      "I'm having a blast chatting with you and coming up with these jokes. If you're ready for more, I\n"
     ]
    }
   ],
   "source": [
    "outputs = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95\n",
    ")\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19f982d7-0825-4d2b-827f-89b56781d104",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T07:29:28.167345Z",
     "iopub.status.busy": "2024-07-21T07:29:28.167160Z",
     "iopub.status.idle": "2024-07-21T07:29:28.174910Z",
     "shell.execute_reply": "2024-07-21T07:29:28.174000Z",
     "shell.execute_reply.started": "2024-07-21T07:29:28.167332Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.626893043518066"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.cuda.max_memory_allocated('cuda:0') +  torch.cuda.max_memory_allocated('cuda:1')) / (1024*1024*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05213867-b8e0-4802-b10e-0e7f87282a90",
   "metadata": {},
   "source": [
    "## GGUF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a8ca34-5496-4a95-b4dd-ba511f8ef78d",
   "metadata": {},
   "source": [
    "- GPT-Generated Unified Format，是由Georgi Gerganov定义发布的一种大模型文件格式。Georgi Gerganov是著名开源项目llama.cpp的创始人。\n",
    "    - GGML：GPT-Generated Model Language\n",
    "- Although GPTQ does compression well, its **focus on GPU** can be a disadvantage if you do not have the hardware to run it.\n",
    "    - GGUF, previously GGML, is a quantization method that allows users to **use the CPU to run an LLM** but also **offload some of its layers to the GPU** for a speed up (`llama.cpp` 中的 `-ngl` ). Although using the CPU is generally slower than using a GPU for inference, it is an incredible format for those running models on CPU or Apple devices.\n",
    "    - Especially since we are seeing smaller and more capable models appearing, like Mistral 7B, the GGUF format might just be here to stay!\n",
    "- Q4_K_M\n",
    "    - Q stands for Quantization.\n",
    "    - 4 indicates the number of bits used in the quantization process.\n",
    "    - K refers to the use of **k-means** clustering in the quantization.\n",
    "    - M represents the size of the model after quantization.\n",
    "        - (S = Small, M = Medium, L = Large)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc056d14-852f-4cbf-a468-0f6f972f6e4a",
   "metadata": {},
   "source": [
    "### quant base kmeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89e4c830-7f7b-489d-b2bd-e85e0487eed0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T07:53:47.429971Z",
     "iopub.status.busy": "2024-07-21T07:53:47.429352Z",
     "iopub.status.idle": "2024-07-21T07:53:47.442054Z",
     "shell.execute_reply": "2024-07-21T07:53:47.439843Z",
     "shell.execute_reply.started": "2024-07-21T07:53:47.429922Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../../imgs/quant-clustering.jpeg\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='../../imgs/quant-clustering.jpeg', width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cc2e315-431f-40ad-8f4f-8cf9483940ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T07:54:38.560190Z",
     "iopub.status.busy": "2024-07-21T07:54:38.559553Z",
     "iopub.status.idle": "2024-07-21T07:54:38.812058Z",
     "shell.execute_reply": "2024-07-21T07:54:38.811210Z",
     "shell.execute_reply.started": "2024-07-21T07:54:38.560142Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "重新排序后的量化索引矩阵：\n",
      " [[3 0 2 1]\n",
      " [1 1 0 3]\n",
      " [0 3 1 0]\n",
      " [3 1 2 2]]\n",
      "重新排序后的质心值：\n",
      " [-1.   0.   1.5  2. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whaow/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 原始权重矩阵\n",
    "weights = np.array([\n",
    "    [2.09, -0.98, 1.48, 0.09],\n",
    "    [0.05, -0.14, -1.08, 2.12],\n",
    "    [-0.91, 1.92, 0, -1.03],\n",
    "    [1.87, 0, 1.53, 1.49]\n",
    "])\n",
    "\n",
    "# K-means聚类\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(weights.reshape(-1, 1))\n",
    "cluster_indices = kmeans.predict(weights.reshape(-1, 1)).reshape(weights.shape)\n",
    "centroids = kmeans.cluster_centers_.flatten()\n",
    "\n",
    "# 根据质心值排序\n",
    "sorted_indices = np.argsort(centroids)\n",
    "sorted_centroids = centroids[sorted_indices]\n",
    "\n",
    "# 创建索引映射\n",
    "index_map = {old_idx: new_idx for new_idx, old_idx in enumerate(sorted_indices)}\n",
    "\n",
    "# 更新量化索引矩阵\n",
    "new_cluster_indices = np.vectorize(index_map.get)(cluster_indices)\n",
    "\n",
    "print(\"重新排序后的量化索引矩阵：\\n\", new_cluster_indices)\n",
    "print(\"重新排序后的质心值：\\n\", sorted_centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1efcb7-5fbf-477e-bac1-a83394963aba",
   "metadata": {},
   "source": [
    "### gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42bb318a-8403-4f55-916f-8cd94964d87c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T07:31:35.044555Z",
     "iopub.status.busy": "2024-07-21T07:31:35.044061Z",
     "iopub.status.idle": "2024-07-21T07:31:35.271228Z",
     "shell.execute_reply": "2024-07-21T07:31:35.269251Z",
     "shell.execute_reply.started": "2024-07-21T07:31:35.044518Z"
    }
   },
   "outputs": [],
   "source": [
    "del tokenizer, model, pipe\n",
    "\n",
    "# Empty VRAM cache\n",
    "import torch\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac6b1001-95cd-4094-9480-6818b072d053",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T07:59:32.981136Z",
     "iopub.status.busy": "2024-07-21T07:59:32.980815Z",
     "iopub.status.idle": "2024-07-21T07:59:33.725088Z",
     "shell.execute_reply": "2024-07-21T07:59:33.723084Z",
     "shell.execute_reply.started": "2024-07-21T07:59:32.981114Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3eb8c430aa4281b5cb403359c08306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 0 files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58c80959a1e40d1b4e5c5442dbbe3ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to create LLM 'gguf' from '/media/whaow/.cache/huggingface/hub/models--QuantFactory--Meta-Llama-3-8B-Instruct-GGUF/blobs/c57380038ea85d8bec586ec2af9c91abc2f2b332d41d6cf180581d7bdffb93c1'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, pipeline\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load LLM and Tokenizer\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Use `gpu_layers` to specify how many layers will be offloaded to the GPU.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQuantFactory/Meta-Llama-3-8B-Instruct-GGUF\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMeta-Llama-3-8B-Instruct.Q4_K_M.gguf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# model_type=\"llama\", \u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpu_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuantFactory/Meta-Llama-3-8B-Instruct-GGUF\u001b[39m\u001b[38;5;124m\"\u001b[39m, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Create a pipeline\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ctransformers/hub.py:175\u001b[0m, in \u001b[0;36mAutoModelForCausalLM.from_pretrained\u001b[0;34m(cls, model_path_or_repo_id, model_type, model_file, config, lib, local_files_only, revision, hf, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    168\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_find_model_path_from_repo(\n\u001b[1;32m    169\u001b[0m         model_path_or_repo_id,\n\u001b[1;32m    170\u001b[0m         model_file,\n\u001b[1;32m    171\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    172\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    173\u001b[0m     )\n\u001b[0;32m--> 175\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m hf:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m llm\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ctransformers/llm.py:253\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model_path, model_type, config, lib)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lib\u001b[38;5;241m.\u001b[39mctransformers_llm_create(\n\u001b[1;32m    248\u001b[0m     model_path\u001b[38;5;241m.\u001b[39mencode(),\n\u001b[1;32m    249\u001b[0m     model_type\u001b[38;5;241m.\u001b[39mencode(),\n\u001b[1;32m    250\u001b[0m     config\u001b[38;5;241m.\u001b[39mto_struct(),\n\u001b[1;32m    251\u001b[0m )\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to create LLM \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m from \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m architecture \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctransformers_llm_architecture()\u001b[38;5;241m.\u001b[39mdecode()\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m architecture:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to create LLM 'gguf' from '/media/whaow/.cache/huggingface/hub/models--QuantFactory--Meta-Llama-3-8B-Instruct-GGUF/blobs/c57380038ea85d8bec586ec2af9c91abc2f2b332d41d6cf180581d7bdffb93c1'."
     ]
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# Load LLM and Tokenizer\n",
    "# Use `gpu_layers` to specify how many layers will be offloaded to the GPU.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"QuantFactory/Meta-Llama-3-8B-Instruct-GGUF\",\n",
    "    model_file=\"Meta-Llama-3-8B-Instruct.Q4_K_M.gguf\",\n",
    "    # model_type=\"llama\", \n",
    "    gpu_layers=20, hf=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"QuantFactory/Meta-Llama-3-8B-Instruct-GGUF\", use_fast=True\n",
    ")\n",
    "\n",
    "# Create a pipeline\n",
    "pipe = pipeline(model=model, tokenizer=tokenizer, task='text-generation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9b01fb-703a-49f8-bf5f-a0ec270e02a3",
   "metadata": {},
   "source": [
    "## AWQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8af6cef-d624-4015-9f92-5735786370bb",
   "metadata": {},
   "source": [
    "\n",
    "A new format on the block is AWQ ([Activation-aware Weight Quantization](https://arxiv.org/abs/2306.00978)) which is a quantization method similar to GPTQ. There are several differences between AWQ and GPTQ as methods but the most important one is that AWQ assumes that **not all weights are equally important for an LLM's performance**.\n",
    "\n",
    "In other words, there is a small fraction of weights that will be skipped during quantization which helps with the quantization loss.\n",
    "\n",
    "As a result, their paper mentions a **significant speed-up compared to GPTQ whilst keeping similar, and sometimes even better, performance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b5cf169-49b4-41dc-9965-22840ca542a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T08:08:29.614127Z",
     "iopub.status.busy": "2024-07-21T08:08:29.613758Z",
     "iopub.status.idle": "2024-07-21T08:08:46.473189Z",
     "shell.execute_reply": "2024-07-21T08:08:46.472358Z",
     "shell.execute_reply.started": "2024-07-21T08:08:29.614108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-21 16:08:30 config.py:241] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 07-21 16:08:30 llm_engine.py:174] Initializing an LLM engine (v0.5.2) with config: model='casperhansen/llama-3-8b-instruct-awq', speculative_config=None, tokenizer='casperhansen/llama-3-8b-instruct-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/llama-3-8b-instruct-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-21 16:08:32 weight_utils.py:218] Using model weights format ['*.safetensors']\n",
      "INFO 07-21 16:08:34 model_runner.py:266] Loading model weights took 5.3440 GB\n",
      "INFO 07-21 16:08:35 gpu_executor.py:86] # GPU blocks: 7977, # CPU blocks: 2048\n",
      "INFO 07-21 16:08:37 model_runner.py:1007] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 07-21 16:08:37 model_runner.py:1011] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 07-21 16:08:46 model_runner.py:1208] Graph capturing finished in 9 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Load the LLM\n",
    "sampling_params = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=256)\n",
    "llm = LLM(\n",
    "    model=\"casperhansen/llama-3-8b-instruct-awq\",\n",
    "    quantization='awq',\n",
    "    dtype='half',\n",
    "    gpu_memory_utilization=.95,\n",
    "    max_model_len=4096\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92f141bc-4f6d-4de9-8dba-b1f5d707d007",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T08:08:51.303703Z",
     "iopub.status.busy": "2024-07-21T08:08:51.303495Z",
     "iopub.status.idle": "2024-07-21T08:08:51.890123Z",
     "shell.execute_reply": "2024-07-21T08:08:51.889288Z",
     "shell.execute_reply.started": "2024-07-21T08:08:51.303689Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"casperhansen/llama-3-8b-instruct-awq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74db154c-ea44-449a-ab57-e980b4c5cf85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T08:08:52.947132Z",
     "iopub.status.busy": "2024-07-21T08:08:52.946892Z",
     "iopub.status.idle": "2024-07-21T08:08:52.958540Z",
     "shell.execute_reply": "2024-07-21T08:08:52.957548Z",
     "shell.execute_reply.started": "2024-07-21T08:08:52.947118Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a friendly chatbot.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Tell me a funny joke about Large Language Models.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Tell me a funny joke about Large Language Models.\"\n",
    "    },\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6ff08c2-d166-4e6f-926e-2466ab8cc186",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T08:08:54.673078Z",
     "iopub.status.busy": "2024-07-21T08:08:54.672489Z",
     "iopub.status.idle": "2024-07-21T08:08:55.341983Z",
     "shell.execute_reply": "2024-07-21T08:08:55.340624Z",
     "shell.execute_reply.started": "2024-07-21T08:08:54.673033Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s, est. speed input: 49.50 toks/s, output: 103.64 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why did the Large Language Model go to therapy?\n",
      "\n",
      "Because it was struggling to \"process\" its emotions and was feeling a little \"dis-connected\" from its users! But in the end, it just needed to \"re-train\" its thinking and \"update\" its perspective!\n",
      "\n",
      "Hope that made you LOL!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate output based on the input prompt and sampling parameters\n",
    "output = llm.generate(prompt, sampling_params)\n",
    "print(output[0].outputs[0].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
