{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cc021e1-7865-4d04-ac37-4c0f2ec60cdc",
   "metadata": {},
   "source": [
    "https://huggingface.co/blog/zh/hf-bitsandbytes-integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bab140d-9f43-4363-815d-a30fdebd9413",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-17T06:37:59.863890Z",
     "iopub.status.busy": "2024-08-17T06:37:59.863244Z",
     "iopub.status.idle": "2024-08-17T06:37:59.872888Z",
     "shell.execute_reply": "2024-08-17T06:37:59.870731Z",
     "shell.execute_reply.started": "2024-08-17T06:37:59.863843Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf50d22-f4c9-4b88-bc1b-ec177d28721b",
   "metadata": {},
   "source": [
    "### precision matters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb0744d-789a-44e1-a5fe-2cfc7e3b633e",
   "metadata": {},
   "source": [
    "- Int8 (INT8) 数据类型，它是一个 8 位的整型数据表示，可以存储 $2^8$ 个不同的值 (对于有符号整数，区间为 [-128, 127]，而对于无符号整数，区间为 [0, 255])。\n",
    "- 虽然理想情况下训练和推理都应该在 FP32 中完成，但 FP32 比 FP16/BF16 慢两倍，因此实践中常常使用混合精度方法，\n",
    "    - 其中，使用 FP32 权重作为精确的 “主权重 (master weight)”，\n",
    "    - 使用 FP16/BF16 权重进行前向和后向传播计算以提高训练速度，最后在梯度更新阶段再使用 FP16/BF16 梯度更新 FP32 主权重。\n",
    "    - 因为只有在模型梯度更新时才需要精确的 FP32 权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92961490-6718-4104-9dc9-46df42afb0c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-17T04:17:07.747331Z",
     "iopub.status.busy": "2024-08-17T04:17:07.746292Z",
     "iopub.status.idle": "2024-08-17T04:17:08.955324Z",
     "shell.execute_reply": "2024-08-17T04:17:08.954494Z",
     "shell.execute_reply.started": "2024-08-17T04:17:07.747294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 result: 100000000.0\n",
      "FP16 result: inf\n",
      "FP16 max value: 65504.0\n",
      "Attempting to represent 100M in FP16: inf\n"
     ]
    }
   ],
   "source": [
    "# 设置 FP32 和 FP16 的张量\n",
    "fp32_tensor = torch.tensor([10000.0], dtype=torch.float32)\n",
    "fp16_tensor = torch.tensor([10000.0], dtype=torch.float16)\n",
    "\n",
    "# 执行乘法运算\n",
    "fp32_result = fp32_tensor * fp32_tensor\n",
    "fp16_result = fp16_tensor * fp16_tensor\n",
    "\n",
    "print(f\"FP32 result: {fp32_result.item()}\")\n",
    "print(f\"FP16 result: {fp16_result.item()}\")\n",
    "\n",
    "# 检查 FP16 的最大值\n",
    "fp16_max = torch.finfo(torch.float16).max\n",
    "print(f\"FP16 max value: {fp16_max}\")\n",
    "\n",
    "# 尝试在 FP16 中表示 100M\n",
    "fp16_overflow = torch.tensor([100000000.0], dtype=torch.float16)\n",
    "print(f\"Attempting to represent 100M in FP16: {fp16_overflow.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1f3b85-7f7a-4132-9fde-9d62f2b933fb",
   "metadata": {},
   "source": [
    "### LLM.int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9c59eb4-19ca-45f9-bf15-febf40856af1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-17T06:36:41.708527Z",
     "iopub.status.busy": "2024-08-17T06:36:41.707881Z",
     "iopub.status.idle": "2024-08-17T06:36:41.727063Z",
     "shell.execute_reply": "2024-08-17T06:36:41.724752Z",
     "shell.execute_reply.started": "2024-08-17T06:36:41.708478Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://huggingface.co/blog/assets/96_hf_bitsandbytes_integration/quant-freeze.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://huggingface.co/blog/assets/96_hf_bitsandbytes_integration/quant-freeze.png', width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09ae423b-ebd9-40dd-bfeb-96a6f14d47df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-17T06:38:05.856434Z",
     "iopub.status.busy": "2024-08-17T06:38:05.855786Z",
     "iopub.status.idle": "2024-08-17T06:38:05.884346Z",
     "shell.execute_reply": "2024-08-17T06:38:05.881953Z",
     "shell.execute_reply.started": "2024-08-17T06:38:05.856386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original fp16 vector: tensor([ 1.2002, -0.5000, -4.3008,  1.2002, -3.0996,  0.7998,  2.4004,  5.3984],\n",
      "       dtype=torch.float16)\n",
      "Quantized int8 vector: tensor([  28,  -12, -101,   28,  -73,   19,   56,  127], dtype=torch.int8)\n",
      "Dequantized fp16 vector: tensor([ 1.1904, -0.5103, -4.2930,  1.1904, -3.1035,  0.8076,  2.3809,  5.3984],\n",
      "       dtype=torch.float16)\n",
      "Quantization factor α: tensor(0.0425, dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "def quantize_and_dequantize(fp16_vector):\n",
    "    # Get max(abs)\n",
    "    max_abs = torch.max(torch.abs(fp16_vector))\n",
    "    \n",
    "    # Calculate quantization factor α\n",
    "    alpha = max_abs / 127.0\n",
    "    \n",
    "    # Quantize to int8\n",
    "    quantized = torch.round(fp16_vector / alpha).clamp(-127, 127).to(torch.int8)\n",
    "    \n",
    "    # Dequantize back to fp16\n",
    "    dequantized = quantized.to(torch.float16) * alpha\n",
    "    \n",
    "    return quantized, dequantized, alpha\n",
    "\n",
    "# Example usage\n",
    "fp16_vector = torch.tensor([1.2, -0.5, -4.3, 1.2, -3.1, 0.8, 2.4, 5.4], dtype=torch.float16)\n",
    "\n",
    "quantized, dequantized, alpha = quantize_and_dequantize(fp16_vector)\n",
    "\n",
    "print(\"Original fp16 vector:\", fp16_vector)\n",
    "print(\"Quantized int8 vector:\", quantized)\n",
    "print(\"Dequantized fp16 vector:\", dequantized)\n",
    "print(\"Quantization factor α:\", alpha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
