{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af7fdcea-79bb-4b00-a78a-8828042a19cf",
   "metadata": {},
   "source": [
    "- https://x.com/karpathy/status/1691571869051445433"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce67792-aaaa-446d-abdb-c32ba111c1eb",
   "metadata": {},
   "source": [
    "llama.cpp surprised many people (myself included) with how quickly you can run large LLMs on small computers, e.g. 7B runs @ ~16 tok/s on a MacBook. Wait don't you need supercomputers to work with LLMs?\n",
    "\n",
    "TLDR at **batch_size=1** (i.e. just generating a single stream of prediction on your computer), the inference is super duper memory-bound. The on-chip compute units are twiddling their thumbs while sucking model weights through a straw from DRAM. Every individual weight that is expensively loaded from DRAM onto the chip is only used for a single instant multiply to process each new input token. So the stat to look at is not FLOPS but the memory bandwidth.\n",
    "\n",
    "Let's take a look:\n",
    "A100: 1935 GB/s memory bandwidth, 1248 TOPS\n",
    "MacBook M2: 100 GB/s, 7 TFLOPS\n",
    "\n",
    "The compute is ~200X but the memory bandwidth only ~20X. So the little M2 chip that could will only be about ~20X slower than a mighty A100. This is ~10X faster than you might naively expect just looking at ops.\n",
    "\n",
    "The situation becomes a lot more different when you inference at a very **high batch size (e.g. ~160+)**, such as when you're hosting an LLM engine simultaneously serving a lot of parallel requests. Or in training, where you aren't forced to go serially token by token and can parallelize across both batch and time dimension, because the next token targets (labels) are known. In these cases, once you load the weights into on-chip cache and pay that large fixed cost, you can re-use them across many input examples and reach ~50%+ utilization, actually making those FLOPS count.\n",
    "\n",
    "So TLDR why is LLM inference surprisingly fast on your MacBook? If all you want to do is batch 1 inference (i.e. a single \"stream\" of generation), only the memory bandwidth matters. And the memory bandwidth gap between chips is a lot smaller, and has been a lot harder to scale compared to flops."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
